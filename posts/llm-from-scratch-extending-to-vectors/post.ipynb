{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM from scratch: vector go brrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "\n",
    "def _add(a: Tensor, b: Tensor):\n",
    "    \"\"\"\n",
    "    Add two tensors\n",
    "    \"\"\"\n",
    "    result = Tensor(a.value + b.value)\n",
    "    result.local_derivatives = (Tensor(1), Tensor(1))\n",
    "    result.args = (a, b)\n",
    "    return result\n",
    "\n",
    "\n",
    "def _sub(a: Tensor, b: Tensor):\n",
    "    \"\"\"\n",
    "    Subtract tensor b from a\n",
    "    \"\"\"\n",
    "    result = Tensor(a.value - b.value)\n",
    "    result.local_derivatives = (Tensor(1), Tensor(-1))\n",
    "    result.args = (a, b)\n",
    "    return result\n",
    "\n",
    "\n",
    "def _mul(a: Tensor, b: Tensor):\n",
    "    \"\"\"\n",
    "    Multiply two tensors\n",
    "    \"\"\"\n",
    "    result = Tensor(a.value * b.value)\n",
    "    result.local_derivatives = (b, a)\n",
    "    result.args = (a, b)\n",
    "    return result\n",
    "\n",
    "\n",
    "class Tensor:\n",
    "    \"\"\"\n",
    "    A float that can be differentiated\n",
    "    \"\"\"\n",
    "\n",
    "    args: tuple[Tensor] = ()\n",
    "    derivative_fns = ()\n",
    "    # The derivative (once we've calculated it).  This is None if the derivative\n",
    "    # has not been computed yet\n",
    "    derivative: Tensor | None = None\n",
    "\n",
    "    def __init__(self, value: float):\n",
    "        self.value = value\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Tensor({self.value.__repr__()})\"\n",
    "\n",
    "    def __eq__(self, other) -> bool:\n",
    "        if not isinstance(other, Tensor):\n",
    "            raise TypeError(f\"Cannot compare a Tensor with a {type(other)}\")\n",
    "        return self.value == other.value\n",
    "\n",
    "    def __add__(self, other) -> Tensor:\n",
    "        if not isinstance(other, Tensor):\n",
    "            raise TypeError(f\"Cannot add a Tensor to a {type(other)}\")\n",
    "        return _add(self, other)\n",
    "\n",
    "    def __sub__(self, other) -> Tensor:\n",
    "        if not isinstance(other, Tensor):\n",
    "            raise TypeError(f\"Cannot subtract a Tensor from a {type(other)}\")\n",
    "        return _sub(self, other)\n",
    "\n",
    "    def __mul__(self, other) -> Tensor:\n",
    "        if not isinstance(other, Tensor):\n",
    "            raise TypeError(f\"Cannot multiply a Tensor with a {type(other)}\")\n",
    "        return _mul(self, other)\n",
    "\n",
    "    def __iadd__(self, other) -> Tensor:\n",
    "        return self.__add__(other)\n",
    "\n",
    "    def __isub__(self, other) -> Tensor:\n",
    "        return self.__sub__(other)\n",
    "\n",
    "    def __imul__(self, other) -> Tensor:\n",
    "        return self.__mul__(other)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Tensor({self.value})\"\n",
    "\n",
    "    def backward(self):\n",
    "        if self.args is None or self.local_derivatives is None:\n",
    "            raise ValueError(\n",
    "                \"Cannot differentiate a Tensor that is not a function of other Tensors\"\n",
    "            )\n",
    "\n",
    "        stack = [(self, Tensor(1))]\n",
    "\n",
    "        while stack:\n",
    "            node, current_derivative = stack.pop()\n",
    "\n",
    "            # if we have reached a parameter (it has no arguments\n",
    "            # because it wasn't created by an operation) then add the\n",
    "            # current_derivative to derivative\n",
    "            if not node.args:\n",
    "                if node.derivative is None:\n",
    "                    node.derivative = current_derivative\n",
    "                else:\n",
    "                    node.derivative += current_derivative\n",
    "                continue\n",
    "\n",
    "            for arg, derivative in zip(node.args, node.local_derivatives):\n",
    "                stack.append((arg, current_derivative * derivative))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last time, we built the core of deep learning system: the automatic differentiation engine. It can differentiate any function you like, provided you only like doing addition, subtraction, multiplication and division. \n",
    "\n",
    "In theory, this is most of what we need. We can add a few new functions like `log` and `exp` and we'll have all of the building blocks that we need to do the calculations for a language model. Lets try adding the most important operation first: matrix multiplication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tensor(0.9687713981380367), Tensor(1.1881056150650755), Tensor(0.8222890444360291)]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def matrix_add(\n",
    "    matrix_1: list[list[Tensor]], \n",
    "    matrix_2: list[list[Tensor]], \n",
    "    ) -> list[list[Tensor]]:\n",
    "\n",
    "    # check that the shapes match\n",
    "    height_1, width_1 = len(matrix_1), len(matrix_1[0])\n",
    "    height_2, width_2 = len(matrix_2), len(matrix_2[0])\n",
    "\n",
    "    assert width_1 == height_2\n",
    "\n",
    "    out = [[Tensor(0) for _ in range(width_2)] for _ in range(height_1)]\n",
    "\n",
    "    for i in range(height_1):\n",
    "        for j in range(width_2):\n",
    "            out[i][j] = matrix_1[i][j] + matrix_2[i][j]\n",
    "\n",
    "    return out\n",
    "\n",
    "size = 100\n",
    "\n",
    "matrix_1 = [[Tensor(random.random()) for _ in range(size)] for _ in range(size)]\n",
    "matrix_2 = [[Tensor(random.random()) for _ in range(size)] for _ in range(size)]\n",
    "\n",
    "result = matrix_add(matrix_1, matrix_2)\n",
    "\n",
    "print(result[0][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the right answer, but there is a problem: it is really expensive.\n",
    "Lets re-run this calculation and track the CPU and RAM usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m result \u001b[38;5;241m=\u001b[39m matrix_add(matrix_1, matrix_2)\n\u001b[1;32m     14\u001b[0m duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[0;32m---> 15\u001b[0m memory_usage \u001b[38;5;241m=\u001b[39m \u001b[43masizeof\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masizeof\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39masizeof\u001b[38;5;241m.\u001b[39masizeof(matrix_1) \u001b[38;5;241m+\u001b[39m asizeof\u001b[38;5;241m.\u001b[39masizeof(matrix_2)\n\u001b[1;32m     16\u001b[0m memory_usage \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m before_memory_usage\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDuration: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnaturaldelta(duration,\u001b[38;5;250m \u001b[39mminimum_unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmilliseconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/bclarkson-code.github.io/.venv/lib/python3.11/site-packages/pympler/asizeof.py:2603\u001b[0m, in \u001b[0;36masizeof\u001b[0;34m(*objs, **opts)\u001b[0m\n\u001b[1;32m   2601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x:  \u001b[38;5;66;03m# don't size, profile or rank _getobjects tuple\u001b[39;00m\n\u001b[1;32m   2602\u001b[0m     _asizer\u001b[38;5;241m.\u001b[39mexclude_objs(t)\n\u001b[0;32m-> 2603\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[43m_asizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masizeof\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2604\u001b[0m _asizer\u001b[38;5;241m.\u001b[39mprint_stats(objs\u001b[38;5;241m=\u001b[39mt, opts\u001b[38;5;241m=\u001b[39mopts)  \u001b[38;5;66;03m# show opts as _kwdstr\u001b[39;00m\n\u001b[1;32m   2605\u001b[0m _asizer\u001b[38;5;241m.\u001b[39m_clear()\n",
      "File \u001b[0;32m~/Documents/bclarkson-code.github.io/.venv/lib/python3.11/site-packages/pympler/asizeof.py:2032\u001b[0m, in \u001b[0;36mAsizer.asizeof\u001b[0;34m(self, *objs, **opts)\u001b[0m\n\u001b[1;32m   2030\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopts)\n\u001b[1;32m   2031\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexclude_refs(\u001b[38;5;241m*\u001b[39mobjs)  \u001b[38;5;66;03m# skip refs to objs\u001b[39;00m\n\u001b[0;32m-> 2032\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/bclarkson-code.github.io/.venv/lib/python3.11/site-packages/pympler/asizeof.py:2032\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2030\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopts)\n\u001b[1;32m   2031\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexclude_refs(\u001b[38;5;241m*\u001b[39mobjs)  \u001b[38;5;66;03m# skip refs to objs\u001b[39;00m\n\u001b[0;32m-> 2032\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m objs)\n",
      "File \u001b[0;32m~/Documents/bclarkson-code.github.io/.venv/lib/python3.11/site-packages/pympler/asizeof.py:1964\u001b[0m, in \u001b[0;36mAsizer._sizer\u001b[0;34m(self, obj, pid, deep, sized)\u001b[0m\n\u001b[1;32m   1962\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# just size and accumulate\u001b[39;00m\n\u001b[1;32m   1963\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m v\u001b[38;5;241m.\u001b[39mrefs(obj, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 1964\u001b[0m         s \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1965\u001b[0m \u001b[38;5;66;03m# deepest recursion reached\u001b[39;00m\n\u001b[1;32m   1966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_depth \u001b[38;5;241m<\u001b[39m d:\n",
      "File \u001b[0;32m~/Documents/bclarkson-code.github.io/.venv/lib/python3.11/site-packages/pympler/asizeof.py:1964\u001b[0m, in \u001b[0;36mAsizer._sizer\u001b[0;34m(self, obj, pid, deep, sized)\u001b[0m\n\u001b[1;32m   1962\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# just size and accumulate\u001b[39;00m\n\u001b[1;32m   1963\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m v\u001b[38;5;241m.\u001b[39mrefs(obj, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 1964\u001b[0m         s \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1965\u001b[0m \u001b[38;5;66;03m# deepest recursion reached\u001b[39;00m\n\u001b[1;32m   1966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_depth \u001b[38;5;241m<\u001b[39m d:\n",
      "    \u001b[0;31m[... skipping similar frames: Asizer._sizer at line 1964 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/bclarkson-code.github.io/.venv/lib/python3.11/site-packages/pympler/asizeof.py:1964\u001b[0m, in \u001b[0;36mAsizer._sizer\u001b[0;34m(self, obj, pid, deep, sized)\u001b[0m\n\u001b[1;32m   1962\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# just size and accumulate\u001b[39;00m\n\u001b[1;32m   1963\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m v\u001b[38;5;241m.\u001b[39mrefs(obj, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 1964\u001b[0m         s \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1965\u001b[0m \u001b[38;5;66;03m# deepest recursion reached\u001b[39;00m\n\u001b[1;32m   1966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_depth \u001b[38;5;241m<\u001b[39m d:\n",
      "File \u001b[0;32m~/Documents/bclarkson-code.github.io/.venv/lib/python3.11/site-packages/pympler/asizeof.py:1941\u001b[0m, in \u001b[0;36mAsizer._sizer\u001b[0;34m(self, obj, pid, deep, sized)\u001b[0m\n\u001b[1;32m   1937\u001b[0m     _typedefs[k] \u001b[38;5;241m=\u001b[39m v \u001b[38;5;241m=\u001b[39m _typedef(obj, derive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_derive_,\n\u001b[1;32m   1938\u001b[0m                                      frames\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_frames_,\n\u001b[1;32m   1939\u001b[0m                                       infer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_)\n\u001b[1;32m   1940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (v\u001b[38;5;241m.\u001b[39mboth \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_code_) \u001b[38;5;129;01mand\u001b[39;00m v\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ign_d:\n\u001b[0;32m-> 1941\u001b[0m     s \u001b[38;5;241m=\u001b[39m f \u001b[38;5;241m=\u001b[39m \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mask\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# flat size\u001b[39;00m\n\u001b[1;32m   1942\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile:\n\u001b[1;32m   1943\u001b[0m         \u001b[38;5;66;03m# profile based on *flat* size\u001b[39;00m\n\u001b[1;32m   1944\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prof(k)\u001b[38;5;241m.\u001b[39mupdate(obj, s)\n",
      "File \u001b[0;32m~/Documents/bclarkson-code.github.io/.venv/lib/python3.11/site-packages/pympler/asizeof.py:1122\u001b[0m, in \u001b[0;36m_Typedef.flat\u001b[0;34m(self, obj, mask)\u001b[0m\n\u001b[1;32m   1120\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleng \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# include items\u001b[39;00m\n\u001b[0;32m-> 1122\u001b[0m     s \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleng\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# workaround sys.getsizeof (and numpy?) bug ... some\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# types are incorrectly sized in some Python versions\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;66;03m# (note, isinstance(obj, ()) == False)\u001b[39;00m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, _getsizeof_excls):\n",
      "File \u001b[0;32m~/Documents/bclarkson-code.github.io/.venv/lib/python3.11/site-packages/pympler/asizeof.py:831\u001b[0m, in \u001b[0;36m_len\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    822\u001b[0m _all_refs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mNone\u001b[39;00m, _cell_refs, _class_refs, _co_refs, _dict_refs, _enum_refs,\n\u001b[1;32m    823\u001b[0m                    _exc_refs, _file_refs, _frame_refs, _func_refs, _gen_refs,\n\u001b[1;32m    824\u001b[0m                    _im_refs, _inst_refs, _iter_refs, _module_refs, _namedtuple_refs,\n\u001b[1;32m    825\u001b[0m                    _prop_refs, _seq_refs, _stat_refs, _statvfs_refs, _tb_refs,\n\u001b[1;32m    826\u001b[0m                    _type_refs, _weak_refs)\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Type-specific length functions\u001b[39;00m\n\u001b[0;32m--> 831\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_len\u001b[39m(obj):\n\u001b[1;32m    832\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Safe len().\u001b[39;00m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m    834\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from humanize import naturalsize, naturaldelta\n",
    "# tracks memory usage\n",
    "from pympler import asizeof\n",
    "\n",
    "matrix_1 = [[Tensor(random.random()) for _ in range(1000)] for _ in range(1000)]\n",
    "matrix_2 = [[Tensor(random.random()) for _ in range(1000)] for _ in range(1000)]\n",
    "\n",
    "start = time.time()\n",
    "before_memory_usage = asizeof.asizeof(matrix_1) + asizeof.asizeof(matrix_2)\n",
    "\n",
    "result = matrix_add(matrix_1, matrix_2)\n",
    "\n",
    "duration = time.time() - start\n",
    "memory_usage = asizeof.asizeof(result) +asizeof.asizeof(matrix_1) + asizeof.asizeof(matrix_2)\n",
    "memory_usage -= before_memory_usage\n",
    "\n",
    "print(f'Duration: {naturaldelta(duration, minimum_unit=\"milliseconds\")}')\n",
    "print(f'Memory usage: {naturalsize(memory_usage)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000144"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.random((1000,1000))\n",
    "b = np.random.random((1000,1000))\n",
    "\n",
    "c = a + b\n",
    "\n",
    "asizeof.asizeof(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning terms, this matrix is tiny. Modern models are usually measured by how many billions of parameters they contain while this matrix has 20,000. We want our model to produce multiple tokens per second which means performing 1000s (depending on the model) of matrix mutliplications per second. Clearly, if we want to build something that fits in memory and trains before the heat death of the universe, some optimisations are needed.\n",
    "\n",
    "Our current approach has some downsides:\n",
    " - Each parameter requires a lot of memory to store\n",
    " - We are using python loops which are very slow\n",
    "\n",
    "That said, any improvement will need to keep the parts of our tensor object that work well. Namely, the automatic differentiation and nice pythonic interface.\n",
    "\n",
    "The answer, as is often the case when trying to make python programs fast, is to use Numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy\n",
    "\n",
    "Technically, I'm breaking my own rules by using numpy. I promised in the first blog post that the LLM would be built in \"vanilla\" python. Numpy is not included in the python standard library so it does not count as \"vanilla\", in some senses of the word. However, it is used to ubiquitously in the python ecosystem that it is vanilla in all but name. Regardless, if I'm not allowed to use numpy then I'd have to effectively recreate it and that would be a waste of everyone's time.\n",
    "\n",
    "Using numpy fixes both of the issues with our current approach. It is very memory efficient and is dramatically faster than doing computations in pure python. If we want to use it, we'll need to figure out how to include numpy arrays into our tensor objects. When using numpy, you do operations on arrays of values instead of on individual values. As a first step, lets get our `Tensor` to use numpy arrays instead of python floats. (We can also rename some of the variables while we're here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    \"\"\"\n",
    "    An array of numbers that can be differentiated\n",
    "    \"\"\"\n",
    "    args: tuple[Tensor] = ()\n",
    "    back_fns: tuple = ()\n",
    "    grad: Tensor | None = None\n",
    "    array: np.ndarray\n",
    "\n",
    "    def __init__(self, array: np.ndarray):\n",
    "        self.array = array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, so good, but what happens to our operations? Originally, we did addition like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add(a: Tensor, b: Tensor):\n",
    "    \"\"\"\n",
    "    Add two tensors\n",
    "    \"\"\"\n",
    "    result = Tensor(a.value + b.value)\n",
    "    result.back_fns = (Tensor(1), Tensor(1))\n",
    "    result.args = (a, b)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to make a couple of changes. First of all, we'll need to change `.value` to `.array`. Provided they are the same shape, you can add numpy arrays in the way as you add python objects so we can leave the `+` as it is. Multiplying a matrix by 1 is the same as doing nothing so we can create a function that does nothing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nothing(tensor: Tensor) -> Tensor:\n",
    "    return tensor\n",
    "\n",
    "def add(tensor_1: Tensor, tensor_2: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Add two tensors\n",
    "    \"\"\"\n",
    "    result = Tensor(tensor_1.array + tensor_2.array)\n",
    "    result.back_fns = (nothing, nothing)\n",
    "    result.args = (tensor_1, tensor_2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try it out and see how much more efficient things are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 0 milliseconds\n",
      "Memory usage: 24.0 MB\n"
     ]
    }
   ],
   "source": [
    "matrix_1 = Tensor(np.random.random((1000,1000))) \n",
    "matrix_2 = Tensor(np.random.random((1000,1000))) \n",
    "\n",
    "start = time.time()\n",
    "before_memory_usage = asizeof.asizeof(matrix_1) + asizeof.asizeof(matrix_2)\n",
    "\n",
    "result = add(matrix_1, matrix_2)\n",
    "\n",
    "duration = time.time() - start\n",
    "memory_usage = asizeof.asizeof(result) +asizeof.asizeof(matrix_1) + asizeof.asizeof(matrix_2)\n",
    "memory_usage -= before_memory_usage\n",
    "\n",
    "print(f'Duration: {naturaldelta(duration, minimum_unit=\"milliseconds\")}')\n",
    "print(f'Memory usage: {naturalsize(memory_usage)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is the easy part. Things get a bit harder when we look at the differentiation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way we could do this is by building a list of every operation we need and then using linear algebra to figure out the derivative for each operation. This works, but has a number of problems:\n",
    " - It takes ages and I don't have the time to plow through all of those derivatives\n",
    " - Matrix derivatives can be tricky. For example you need to care about row vs column vectors\n",
    " - The equations very quickly get long and complex which is both frustrating to deal with and error prone\n",
    "\n",
    "Instead, there is a much more elegant solution: the einsum operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ein-what?\n",
    "Standard linear algebra notation usually looks something like this:\n",
    "$$\\textbf{z} = c \\cdot \\textbf{x}^{T}A\\textbf{x}$$\n",
    "\n",
    "You can tell what type of object a given symbol represents by what it looks like:\n",
    " - scalars are normal text\n",
    " - vectors are bold\n",
    " - matrices are capitalised\n",
    "\n",
    "If two symbols are next to each other, then they are matrix multiplied together. Because matrix multiplication depends on the shape of each symbol, we have to keep track of whether a vector is horizontal or vertical (row or column) to make sure that each matrix multiplication is allowed.\n",
    "\n",
    "This system is usable, but difficult. The normal rules of high-school algebra don't always apply and you have to be a lot more careful about what you are doing. When we start including derivatives, this only gets harder.\n",
    "\n",
    "Instead, there is an alternative system which end up being a lot simpler and easier to use.\n",
    "\n",
    "We can imagine giving each element in an object an index (which row is it on, which column etc)\n",
    "$$\\begin{pmatrix}\n",
    "a_1\\\\ \n",
    "...\\\\ \n",
    "a_n\n",
    "\\end{pmatrix} = \\textbf{a}$$\n",
    "\n",
    "Then, we can invent a dummy variable that stands for one of these indices:\n",
    "$$\\begin{pmatrix}\n",
    "a_1\\\\ \n",
    "...\\\\ \n",
    "a_n\n",
    "\\end{pmatrix} = \\textbf{a} = a_{i}$$\n",
    "\n",
    "Whenever we set $i$ to an actual value, we get one of the elements of the vector.\n",
    "This works for tensors of any shape:\n",
    "$$\\begin{pmatrix}\n",
    "a_1 & ... & a_n\\\\ \n",
    "\\vdots &  \\ddots & \\vdots\\\\ \n",
    "z_1 & ... & z_n\n",
    "\\end{pmatrix} = B = b_{ij}$$\n",
    "$$\\begin{pmatrix}\n",
    "(4,2,6)&(8,2,6)&(2,2,5)\\\\\n",
    "(3,3,0)&(8,9,2)&(0,0,3)\\\\\n",
    "(3,0,2)&(6,8,3)&(7,7,5)\\\\\n",
    "\\end{pmatrix} = C = c_{ijk}$$\n",
    "\n",
    "Note that each dimension gets its own letter. It doesn't matter which letters you choose, provided each dimension has a different one.\n",
    "\n",
    "This notation isn't very helpful on its own, but becomes very useful when we add some rules:\n",
    "> If two objects have the same letter for their index, we multiply each value along these shared incides\n",
    "\n",
    "For example, if we have:\n",
    "$$\\begin{pmatrix}\n",
    "1&2&3\\\\\n",
    "4&5&6\n",
    "\\end{pmatrix} = a_{ij}$$\n",
    "and \n",
    "$$\\begin{pmatrix}\n",
    "1&2\\\\\n",
    "3&4\\\\\n",
    "5&6\n",
    "\\end{pmatrix} = b_{jk}$$\n",
    "then\n",
    "$$a_{ij}b_{jk} = $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,2,6)&(8,2,6)&(2,2,5)\\\\\n",
      "(3,3,0)&(8,9,2)&(0,0,3)\\\\\n",
      "(3,0,2)&(6,8,3)&(7,7,5)\\\\\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "s = \"\"\"\"\"\"\n",
    "for row in range(1,4):\n",
    "    column = []\n",
    "    for col in range(1,4):\n",
    "        item = []\n",
    "        for idx in range(1,4):\n",
    "            item.append(str(random.randint(0, 9)))\n",
    "        item = f\"({','.join(item)})\"\n",
    "        column.append(item)\n",
    "    column = f\"{'&'.join(column)}\\\\\\\\\"\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([46, 94])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.arange(6).reshape(3, 2)\n",
    "b = np.arange(6, 12).reshape(2, 3)\n",
    "\n",
    "np.einsum(\"ij,ji->j\", a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: examples of using einsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Midddle bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a tensor from last time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "class Tensor:\n",
    "    \"\"\"\n",
    "    A float that can be differentiated\n",
    "    \"\"\"\n",
    "\n",
    "    args: tuple[Tensor] = ()\n",
    "    derivative_fns: tuple[Op] = ()\n",
    "    # The derivative (once we've calculated it).  This is None if the derivative\n",
    "    # has not been computed yet\n",
    "    derivative: Tensor | None = None\n",
    "\n",
    "    def __init__(self, value: float):\n",
    "        self.value = value\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Tensor({self.value.__repr__()})\"\n",
    "\n",
    "    def __eq__(self, other) -> bool:\n",
    "        if not isinstance(other, Tensor):\n",
    "            raise TypeError(f\"Cannot compare a Tensor with a {type(other)}\")\n",
    "        return self.value == other.value\n",
    "\n",
    "    def __add__(self, other) -> Tensor:\n",
    "        if not isinstance(other, Tensor):\n",
    "            raise TypeError(f\"Cannot add a Tensor to a {type(other)}\")\n",
    "        return _add(self, other)\n",
    "\n",
    "    def __sub__(self, other) -> Tensor:\n",
    "        if not isinstance(other, Tensor):\n",
    "            raise TypeError(f\"Cannot subtract a Tensor from a {type(other)}\")\n",
    "        return _sub(self, other)\n",
    "\n",
    "    def __mul__(self, other) -> Tensor:\n",
    "        if not isinstance(other, Tensor):\n",
    "            raise TypeError(f\"Cannot multiply a Tensor with a {type(other)}\")\n",
    "        return _mul(self, other)\n",
    "\n",
    "    def __iadd__(self, other) -> Tensor:\n",
    "        return self.__add__(other)\n",
    "\n",
    "    def __isub__(self, other) -> Tensor:\n",
    "        return self.__sub__(other)\n",
    "\n",
    "    def __imul__(self, other) -> Tensor:\n",
    "        return self.__mul__(other)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Tensor({self.value})\"\n",
    "\n",
    "    def backward(self):\n",
    "        if self.args is None or self.local_derivatives is None:\n",
    "            raise ValueError(\n",
    "                \"Cannot differentiate a Tensor that is not a function of other Tensors\"\n",
    "            )\n",
    "\n",
    "        stack = [(self, Tensor(1))]\n",
    "\n",
    "        while stack:\n",
    "            node, current_derivative = stack.pop()\n",
    "\n",
    "            # if we have reached a parameter (it has no arguments\n",
    "            # because it wasn't created by an operation) then add the\n",
    "            # current_derivative to derivative\n",
    "            if not node.args:\n",
    "                if node.derivative is None:\n",
    "                    node.derivative = current_derivative\n",
    "                else:\n",
    "                    node.derivative += current_derivative\n",
    "                continue\n",
    "\n",
    "            for arg, derivative in zip(node.args, node.local_derivatives):\n",
    "                stack.append((arg, current_derivative * derivative))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can no longer multiply values, instead we need to apply functions (to handle einsums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    \"\"\"\n",
    "    A float that can be differentiated\n",
    "    \"\"\"\n",
    "\n",
    "    args: tuple[Tensor] = ()\n",
    "    derivative_fns: tuple[Op] = ()\n",
    "    # The derivative (once we've calculated it).  This is None if the derivative\n",
    "    # has not been computed yet\n",
    "    derivative: Tensor | None = None\n",
    "\n",
    "    def __init__(self, value: float):\n",
    "        self.value = value\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Tensor({self.value.__repr__()})\"\n",
    "\n",
    "    def __eq__(self, other) -> bool:\n",
    "        if not isinstance(other, Tensor):\n",
    "            raise TypeError(f\"Cannot compare a Tensor with a {type(other)}\")\n",
    "        return self.value == other.value\n",
    "\n",
    "    def __add__(self, other) -> Tensor:\n",
    "        if not isinstance(other, Tensor):\n",
    "            raise TypeError(f\"Cannot add a Tensor to a {type(other)}\")\n",
    "        return _add(self, other)\n",
    "\n",
    "    def __sub__(self, other) -> Tensor:\n",
    "        if not isinstance(other, Tensor):\n",
    "            raise TypeError(f\"Cannot subtract a Tensor from a {type(other)}\")\n",
    "        return _sub(self, other)\n",
    "\n",
    "    def __mul__(self, other) -> Tensor:\n",
    "        if not isinstance(other, Tensor):\n",
    "            raise TypeError(f\"Cannot multiply a Tensor with a {type(other)}\")\n",
    "        return _mul(self, other)\n",
    "\n",
    "    def __iadd__(self, other) -> Tensor:\n",
    "        return self.__add__(other)\n",
    "\n",
    "    def __isub__(self, other) -> Tensor:\n",
    "        return self.__sub__(other)\n",
    "\n",
    "    def __imul__(self, other) -> Tensor:\n",
    "        return self.__mul__(other)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Tensor({self.value})\"\n",
    "\n",
    "    def backward(self):\n",
    "        if self.args is None or self.local_derivatives is None:\n",
    "            raise ValueError(\n",
    "                \"Cannot differentiate a Tensor that is not a function of other Tensors\"\n",
    "            )\n",
    "\n",
    "        stack = [(self, Tensor(1))]\n",
    "\n",
    "        while stack:\n",
    "            node, current_derivative = stack.pop()\n",
    "\n",
    "            # if we have reached a parameter (it has no arguments\n",
    "            # because it wasn't created by an operation) then add the\n",
    "            # current_derivative to derivative\n",
    "            if not node.args:\n",
    "                if node.derivative is None:\n",
    "                    node.derivative = current_derivative\n",
    "                else:\n",
    "                    node.derivative += current_derivative\n",
    "                continue\n",
    "\n",
    "            for arg, derivative_fn in zip(node.args, node.derivative_fns):\n",
    "                stack.append((arg, derivative_fn(current_derivative)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This won't work because we haven't defined `_add`, `_sub` or `_mul`. Lets define them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Unary Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mul(a: Tensor, b: Tensor):\n",
    "    \"\"\"\n",
    "    Multiply two tensors\n",
    "    \"\"\"\n",
    "    result = Tensor(a.value * b.value)\n",
    "    result.local_derivatives = (b, a)\n",
    "    result.args = (a, b)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def _umul(tensor: Tensor, constant: np.number) -> Tensor:\n",
    "    assert isinstance(tensor, Tensor)\n",
    "    assert np.isscalar(constant)\n",
    "\n",
    "    result = Tensor(tensor.value * constant)\n",
    "    result.derivative_fns = ()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class einsum:\n",
    "    def __init__(self, subscript: str):\n",
    "        self.subscript = subscript\n",
    "        self.indices, self.output = _parse_subscripts(subscript)\n",
    "\n",
    "    def _generate_back_fns(self, tensors: Sequence[Tensor]):\n",
    "        assert len(tensors) == len(self.indices)\n",
    "        back_functions = []\n",
    "        for idx in range(len(tensors)):\n",
    "\n",
    "            def back_fn(tensor: Tensor, idx: int = idx):\n",
    "                left_tensors = tensors[:idx]\n",
    "                left_subscript = self.indices[:idx]\n",
    "\n",
    "                right_tensors = tensors[idx + 1 :] if idx < len(tensors) - 1 else []\n",
    "                right_subscript = (\n",
    "                    self.indices[idx + 1 :] if idx < len(tensors) - 1 else []\n",
    "                )\n",
    "\n",
    "                subscript = left_subscript + [self.output] + right_subscript\n",
    "                subscript = \",\".join(subscript) + \"->\" + self.indices[idx]\n",
    "\n",
    "                fn_args = [*left_tensors, tensor, *right_tensors]\n",
    "                return einsum(subscript)(*fn_args)\n",
    "\n",
    "            back_functions.append(back_fn)\n",
    "\n",
    "        return back_functions\n",
    "\n",
    "    def __call__(self, *tensors: Tensor):\n",
    "        result = to_tensor(np.einsum(self.subscript, *tensors))\n",
    "        result.args = tuple(tensors)\n",
    "        result.back_fn = tuple(self._generate_back_fns(tensors))\n",
    "        result.name = f\"einsum {self.subscript}\"\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Proof\n",
    "\n",
    "Let $A, B,...$ be a collection of tensors of arbitrary dimension.\n",
    "\n",
    "Suppose these tensors are combined with a valid einsum operation as follows (in einstein notation):\n",
    "\n",
    "$$z_{z_1, z_2, ...,z_n} = a_{a_1, a_2, ...,a_m}b_{b_1, b_2,...,b_p}...$$\n",
    "\n",
    "Then, because the einsum operation is valid:\n",
    "\n",
    "$$\\{z_1, z_2, ...,z_n\\} \\subset \\{a_1, a_2, ...,a_m,b_1, b_2,...,b_p,...\\}$$\n",
    "\n",
    "That is, every index in the output tensor is included at least once in the set of indices for input tensors.\n",
    "\n",
    "Note here that while the set of indices for each individual tensor contains no duplicates: \n",
    "$$|\\{z_1, z_2, ...,z_n\\}| = n$$\n",
    "$$|\\{a_1, a_2, ...,a_m\\}| = m$$\n",
    "$$|\\{b_1, b_2, ...,b_p\\}| = p$$\n",
    "The same index can appear in the set of indices for multiple input tensors.\n",
    "\n",
    "The einsum operation can be written in terms of an `einsum` function as follows:\n",
    "\n",
    "$$Z = \\texttt{einsum}(a_1, a_2, ...,a_m,b_1, b_2,...,b_p,...\\rightarrow z_1, z_2, ...,z_n)(A, B,...)$$\n",
    "\n",
    "\n",
    "Now suppose that each input is potentially a function of some value $x$: $A(x), B(x), ...$ (although the derivative of each input wrt $x$ could be $0$) . In einstein notation, this becomes:\n",
    "\n",
    "$$z_{z_1, z_2, ...,z_n}(x) = a_{a_1, a_2, ...,a_m}(x)b_{b_1, b_2,...,b_p}(x)...$$\n",
    "\n",
    "Thus, we find \n",
    "$$\\frac{\\partial Z}{\\partial x} = \\frac{\\partial z_{z_1, z_2, ...,z_n}(x)}{\\partial x} = \\frac{\\partial }{\\partial x}\\left ( a_{a_1, a_2, ...,a_m}b_{b_1, b_2,...,b_p}... \\right )$$\n",
    "\n",
    "Because we are using einstein notation, we can apply the product rule to get:\n",
    "\n",
    "$$\\frac{\\partial }{\\partial x}\\left ( a_{a_1, a_2, ...,a_m}b_{b_1, b_2,...,b_p}... \\right ) = \\frac{\\partial a_{a_1, a_2, ...,a_m}(x)}{\\partial x}b_{b_1, b_2,...,b_p}... + a_{a_1, a_2, ...,a_m}(x)\\frac{\\partial b_{b_1, b_2,...,b_p}}{\\partial x}... + ...$$\n",
    "\n",
    "Using the `einstein` function, we can rewrite this as\n",
    "\n",
    "$$\\frac{\\partial Z}{\\partial x}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
