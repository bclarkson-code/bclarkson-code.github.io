{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM from scratch: vector go brrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "\n",
    "def _add(a: Tensor, b: Tensor):\n",
    "    \"\"\"\n",
    "    Add two tensors\n",
    "    \"\"\"\n",
    "    result = Tensor(a.value + b.value)\n",
    "    result.local_derivatives = (Tensor(1), Tensor(1))\n",
    "    result.args = (a, b)\n",
    "    return result\n",
    "\n",
    "\n",
    "def _sub(a: Tensor, b: Tensor):\n",
    "    \"\"\"\n",
    "    Subtract tensor b from a\n",
    "    \"\"\"\n",
    "    result = Tensor(a.value - b.value)\n",
    "    result.local_derivatives = (Tensor(1), Tensor(-1))\n",
    "    result.args = (a, b)\n",
    "    return result\n",
    "\n",
    "\n",
    "def _mul(a: Tensor, b: Tensor):\n",
    "    \"\"\"\n",
    "    Multiply two tensors\n",
    "    \"\"\"\n",
    "    result = Tensor(a.value * b.value)\n",
    "    result.local_derivatives = (b, a)\n",
    "    result.args = (a, b)\n",
    "    return result\n",
    "\n",
    "\n",
    "class Tensor:\n",
    "    \"\"\"\n",
    "    A float that can be differentiated\n",
    "    \"\"\"\n",
    "\n",
    "    args: tuple[Tensor] = ()\n",
    "    derivative_fns = ()\n",
    "    # The derivative (once we've calculated it).  This is None if the derivative\n",
    "    # has not been computed yet\n",
    "    derivative: Tensor | None = None\n",
    "\n",
    "    def __init__(self, value: float):\n",
    "        self.value = value\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Tensor({self.value.__repr__()})\"\n",
    "\n",
    "    def __eq__(self, other) -> bool:\n",
    "        if not isinstance(other, Tensor):\n",
    "            raise TypeError(f\"Cannot compare a Tensor with a {type(other)}\")\n",
    "        return self.value == other.value\n",
    "\n",
    "    def __add__(self, other) -> Tensor:\n",
    "        if not isinstance(other, Tensor):\n",
    "            raise TypeError(f\"Cannot add a Tensor to a {type(other)}\")\n",
    "        return _add(self, other)\n",
    "\n",
    "    def __sub__(self, other) -> Tensor:\n",
    "        if not isinstance(other, Tensor):\n",
    "            raise TypeError(f\"Cannot subtract a Tensor from a {type(other)}\")\n",
    "        return _sub(self, other)\n",
    "\n",
    "    def __mul__(self, other) -> Tensor:\n",
    "        if not isinstance(other, Tensor):\n",
    "            raise TypeError(f\"Cannot multiply a Tensor with a {type(other)}\")\n",
    "        return _mul(self, other)\n",
    "\n",
    "    def __iadd__(self, other) -> Tensor:\n",
    "        return self.__add__(other)\n",
    "\n",
    "    def __isub__(self, other) -> Tensor:\n",
    "        return self.__sub__(other)\n",
    "\n",
    "    def __imul__(self, other) -> Tensor:\n",
    "        return self.__mul__(other)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Tensor({self.value})\"\n",
    "\n",
    "    def backward(self):\n",
    "        if self.args is None or self.local_derivatives is None:\n",
    "            raise ValueError(\n",
    "                \"Cannot differentiate a Tensor that is not a function of other Tensors\"\n",
    "            )\n",
    "\n",
    "        stack = [(self, Tensor(1))]\n",
    "\n",
    "        while stack:\n",
    "            node, current_derivative = stack.pop()\n",
    "\n",
    "            # if we have reached a parameter (it has no arguments\n",
    "            # because it wasn't created by an operation) then add the\n",
    "            # current_derivative to derivative\n",
    "            if not node.args:\n",
    "                if node.derivative is None:\n",
    "                    node.derivative = current_derivative\n",
    "                else:\n",
    "                    node.derivative += current_derivative\n",
    "                continue\n",
    "\n",
    "            for arg, derivative in zip(node.args, node.local_derivatives):\n",
    "                stack.append((arg, current_derivative * derivative))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last time, we built the core of deep learning system: the automatic differentiation engine. It can differentiate any function you like, provided you only like doing addition, subtraction, multiplication and division. \n",
    "\n",
    "In theory, this is most of what we need. We can add a few new functions like `log` and `exp` and we'll have all of the building blocks that we need to do the calculations for a language model. Lets try adding the most important operation first: matrix multiplication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tensor(25.117550511440133), Tensor(25.083568426256015), Tensor(29.174617381383054)]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def matrix_multiply(\n",
    "    matrix_1: list[list[Tensor]], \n",
    "    matrix_2: list[list[Tensor]], \n",
    "    ) -> list[list[Tensor]]:\n",
    "\n",
    "    # check that the shapes match\n",
    "    height_1, width_1 = len(matrix_1), len(matrix_1[0])\n",
    "    height_2, width_2 = len(matrix_2), len(matrix_2[0])\n",
    "\n",
    "    assert width_1 == height_2\n",
    "\n",
    "    out = [[Tensor(0) for _ in range(width_2)] for _ in range(height_1)]\n",
    "\n",
    "    for i in range(height_1):\n",
    "        for j in range(width_2):\n",
    "            for k in range(width_2):\n",
    "                out[i][j] += matrix_1[i][k] * matrix_2[k][j]\n",
    "\n",
    "    return out\n",
    "\n",
    "size = 100\n",
    "\n",
    "matrix_1 = [[Tensor(random.random()) for _ in range(size)] for _ in range(size)]\n",
    "matrix_2 = [[Tensor(random.random()) for _ in range(size)] for _ in range(size)]\n",
    "\n",
    "result = matrix_multiply(matrix_1, matrix_2)\n",
    "\n",
    "print(result[0][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the right answer, but there is a problem. As I am sure you are aware, deep learning models involve a whole lot of matrix multiplication. A single dense layer from one of the models we'll build later multiplies a 1536 x 384 matrix by a 384 x 256 matrix. Lets see how long this would take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "matrix_1 = [[Tensor(random.random()) for _ in range(384)] for _ in range(1536)]\n",
    "matrix_2 = [[Tensor(random.random()) for _ in range(256)] for _ in range(384)]\n",
    "\n",
    "%timeit matrix_multiply(matrix_1, matrix_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "875 µs ± 380 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "a = np.random.random((100,100))\n",
    "b = np.random.random((100,100))\n",
    "\n",
    "%timeit np.matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 µs ± 1.82 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.arange(1000)\n",
    "b = np.arange(1000)\n",
    "\n",
    "%timeit np.dot(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way we could do this is by building a list of every operation we need and then using linear algebra to figure out the derivative for each operation. This works, but has a number of problems:\n",
    " - It takes ages and I don't have the time to plow through all of those derivatives\n",
    " - Matrix derivatives can be tricky. For example you need to care about row vs column vectors\n",
    " - The equations very quickly get long and complex which is both frustrating to deal with and error prone\n",
    "\n",
    "Instead, there is a much more elegant solution: the einsum operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ein-what?\n",
    "Standard linear algebra notation usually looks something like this:\n",
    "$$\\textbf{z} = c \\cdot \\textbf{x}^{T}A\\textbf{x}$$\n",
    "\n",
    "You can tell what type of object a given symbol represents by what it looks like:\n",
    " - scalars are normal text\n",
    " - vectors are bold\n",
    " - matrices are capitalised\n",
    "\n",
    "If two symbols are next to each other, then they are matrix multiplied together. Because matrix multiplication depends on the shape of each symbol, we have to keep track of whether a vector is horizontal or vertical (row or column) to make sure that each matrix multiplication is allowed.\n",
    "\n",
    "This system is usable, but difficult. The normal rules of high-school algebra don't always apply and you have to be a lot more careful about what you are doing. When we start including derivatives, this only gets harder.\n",
    "\n",
    "Instead, there is an alternative system which end up being a lot simpler and easier to use.\n",
    "\n",
    "We can imagine giving each element in an object an index (which row is it on, which column etc)\n",
    "$$\\begin{pmatrix}\n",
    "a_1\\\\ \n",
    "...\\\\ \n",
    "a_n\n",
    "\\end{pmatrix} = \\textbf{a}$$\n",
    "\n",
    "Then, we can invent a dummy variable that stands for one of these indices:\n",
    "$$\\begin{pmatrix}\n",
    "a_1\\\\ \n",
    "...\\\\ \n",
    "a_n\n",
    "\\end{pmatrix} = \\textbf{a} = a_{i}$$\n",
    "\n",
    "Whenever we set $i$ to an actual value, we get one of the elements of the vector.\n",
    "This works for tensors of any shape:\n",
    "$$\\begin{pmatrix}\n",
    "a_1 & ... & a_n\\\\ \n",
    "\\vdots &  \\ddots & \\vdots\\\\ \n",
    "z_1 & ... & z_n\n",
    "\\end{pmatrix} = B = b_{ij}$$\n",
    "$$\\begin{pmatrix}\n",
    "(4,2,6)&(8,2,6)&(2,2,5)\\\\\n",
    "(3,3,0)&(8,9,2)&(0,0,3)\\\\\n",
    "(3,0,2)&(6,8,3)&(7,7,5)\\\\\n",
    "\\end{pmatrix} = C = c_{ijk}$$\n",
    "\n",
    "Note that each dimension gets its own letter. It doesn't matter which letters you choose, provided each dimension has a different one.\n",
    "\n",
    "This notation isn't very helpful on its own, but becomes very useful when we add some rules:\n",
    "> If two objects have the same letter for their index, we multiply each value along these shared incides\n",
    "\n",
    "For example, if we have:\n",
    "$$\\begin{pmatrix}\n",
    "1&2&3\\\\\n",
    "4&5&6\n",
    "\\end{pmatrix} = a_{ij}$$\n",
    "and \n",
    "$$\\begin{pmatrix}\n",
    "1&2\\\\\n",
    "3&4\\\\\n",
    "5&6\n",
    "\\end{pmatrix} = b_{jk}$$\n",
    "then\n",
    "$$a_{ij}b_{jk} = $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,2,6)&(8,2,6)&(2,2,5)\\\\\n",
      "(3,3,0)&(8,9,2)&(0,0,3)\\\\\n",
      "(3,0,2)&(6,8,3)&(7,7,5)\\\\\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "s = \"\"\"\"\"\"\n",
    "for row in range(1,4):\n",
    "    column = []\n",
    "    for col in range(1,4):\n",
    "        item = []\n",
    "        for idx in range(1,4):\n",
    "            item.append(str(random.randint(0, 9)))\n",
    "        item = f\"({','.join(item)})\"\n",
    "        column.append(item)\n",
    "    column = f\"{'&'.join(column)}\\\\\\\\\"\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([46, 94])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.arange(6).reshape(3, 2)\n",
    "b = np.arange(6, 12).reshape(2, 3)\n",
    "\n",
    "np.einsum(\"ij,ji->j\", a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: examples of using einsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Midddle bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a tensor from last time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "class Tensor:\n",
    "    \"\"\"\n",
    "    A float that can be differentiated\n",
    "    \"\"\"\n",
    "\n",
    "    args: tuple[Tensor] = ()\n",
    "    derivative_fns: tuple[Op] = ()\n",
    "    # The derivative (once we've calculated it).  This is None if the derivative\n",
    "    # has not been computed yet\n",
    "    derivative: Tensor | None = None\n",
    "\n",
    "    def __init__(self, value: float):\n",
    "        self.value = value\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Tensor({self.value.__repr__()})\"\n",
    "\n",
    "    def __eq__(self, other) -> bool:\n",
    "        if not isinstance(other, Tensor):\n",
    "            raise TypeError(f\"Cannot compare a Tensor with a {type(other)}\")\n",
    "        return self.value == other.value\n",
    "\n",
    "    def __add__(self, other) -> Tensor:\n",
    "        if not isinstance(other, Tensor):\n",
    "            raise TypeError(f\"Cannot add a Tensor to a {type(other)}\")\n",
    "        return _add(self, other)\n",
    "\n",
    "    def __sub__(self, other) -> Tensor:\n",
    "        if not isinstance(other, Tensor):\n",
    "            raise TypeError(f\"Cannot subtract a Tensor from a {type(other)}\")\n",
    "        return _sub(self, other)\n",
    "\n",
    "    def __mul__(self, other) -> Tensor:\n",
    "        if not isinstance(other, Tensor):\n",
    "            raise TypeError(f\"Cannot multiply a Tensor with a {type(other)}\")\n",
    "        return _mul(self, other)\n",
    "\n",
    "    def __iadd__(self, other) -> Tensor:\n",
    "        return self.__add__(other)\n",
    "\n",
    "    def __isub__(self, other) -> Tensor:\n",
    "        return self.__sub__(other)\n",
    "\n",
    "    def __imul__(self, other) -> Tensor:\n",
    "        return self.__mul__(other)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Tensor({self.value})\"\n",
    "\n",
    "    def backward(self):\n",
    "        if self.args is None or self.local_derivatives is None:\n",
    "            raise ValueError(\n",
    "                \"Cannot differentiate a Tensor that is not a function of other Tensors\"\n",
    "            )\n",
    "\n",
    "        stack = [(self, Tensor(1))]\n",
    "\n",
    "        while stack:\n",
    "            node, current_derivative = stack.pop()\n",
    "\n",
    "            # if we have reached a parameter (it has no arguments\n",
    "            # because it wasn't created by an operation) then add the\n",
    "            # current_derivative to derivative\n",
    "            if not node.args:\n",
    "                if node.derivative is None:\n",
    "                    node.derivative = current_derivative\n",
    "                else:\n",
    "                    node.derivative += current_derivative\n",
    "                continue\n",
    "\n",
    "            for arg, derivative in zip(node.args, node.local_derivatives):\n",
    "                stack.append((arg, current_derivative * derivative))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can no longer multiply values, instead we need to apply functions (to handle einsums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    \"\"\"\n",
    "    A float that can be differentiated\n",
    "    \"\"\"\n",
    "\n",
    "    args: tuple[Tensor] = ()\n",
    "    derivative_fns: tuple[Op] = ()\n",
    "    # The derivative (once we've calculated it).  This is None if the derivative\n",
    "    # has not been computed yet\n",
    "    derivative: Tensor | None = None\n",
    "\n",
    "    def __init__(self, value: float):\n",
    "        self.value = value\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Tensor({self.value.__repr__()})\"\n",
    "\n",
    "    def __eq__(self, other) -> bool:\n",
    "        if not isinstance(other, Tensor):\n",
    "            raise TypeError(f\"Cannot compare a Tensor with a {type(other)}\")\n",
    "        return self.value == other.value\n",
    "\n",
    "    def __add__(self, other) -> Tensor:\n",
    "        if not isinstance(other, Tensor):\n",
    "            raise TypeError(f\"Cannot add a Tensor to a {type(other)}\")\n",
    "        return _add(self, other)\n",
    "\n",
    "    def __sub__(self, other) -> Tensor:\n",
    "        if not isinstance(other, Tensor):\n",
    "            raise TypeError(f\"Cannot subtract a Tensor from a {type(other)}\")\n",
    "        return _sub(self, other)\n",
    "\n",
    "    def __mul__(self, other) -> Tensor:\n",
    "        if not isinstance(other, Tensor):\n",
    "            raise TypeError(f\"Cannot multiply a Tensor with a {type(other)}\")\n",
    "        return _mul(self, other)\n",
    "\n",
    "    def __iadd__(self, other) -> Tensor:\n",
    "        return self.__add__(other)\n",
    "\n",
    "    def __isub__(self, other) -> Tensor:\n",
    "        return self.__sub__(other)\n",
    "\n",
    "    def __imul__(self, other) -> Tensor:\n",
    "        return self.__mul__(other)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Tensor({self.value})\"\n",
    "\n",
    "    def backward(self):\n",
    "        if self.args is None or self.local_derivatives is None:\n",
    "            raise ValueError(\n",
    "                \"Cannot differentiate a Tensor that is not a function of other Tensors\"\n",
    "            )\n",
    "\n",
    "        stack = [(self, Tensor(1))]\n",
    "\n",
    "        while stack:\n",
    "            node, current_derivative = stack.pop()\n",
    "\n",
    "            # if we have reached a parameter (it has no arguments\n",
    "            # because it wasn't created by an operation) then add the\n",
    "            # current_derivative to derivative\n",
    "            if not node.args:\n",
    "                if node.derivative is None:\n",
    "                    node.derivative = current_derivative\n",
    "                else:\n",
    "                    node.derivative += current_derivative\n",
    "                continue\n",
    "\n",
    "            for arg, derivative_fn in zip(node.args, node.derivative_fns):\n",
    "                stack.append((arg, derivative_fn(current_derivative)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This won't work because we haven't defined `_add`, `_sub` or `_mul`. Lets define them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Unary Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mul(a: Tensor, b: Tensor):\n",
    "    \"\"\"\n",
    "    Multiply two tensors\n",
    "    \"\"\"\n",
    "    result = Tensor(a.value * b.value)\n",
    "    result.local_derivatives = (b, a)\n",
    "    result.args = (a, b)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def _umul(tensor: Tensor, constant: np.number) -> Tensor:\n",
    "    assert isinstance(tensor, Tensor)\n",
    "    assert np.isscalar(constant)\n",
    "\n",
    "    result = Tensor(tensor.value * constant)\n",
    "    result.derivative_fns = ()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class einsum:\n",
    "    def __init__(self, subscript: str):\n",
    "        self.subscript = subscript\n",
    "        self.indices, self.output = _parse_subscripts(subscript)\n",
    "\n",
    "    def _generate_back_fns(self, tensors: Sequence[Tensor]):\n",
    "        assert len(tensors) == len(self.indices)\n",
    "        back_functions = []\n",
    "        for idx in range(len(tensors)):\n",
    "\n",
    "            def back_fn(tensor: Tensor, idx: int = idx):\n",
    "                left_tensors = tensors[:idx]\n",
    "                left_subscript = self.indices[:idx]\n",
    "\n",
    "                right_tensors = tensors[idx + 1 :] if idx < len(tensors) - 1 else []\n",
    "                right_subscript = (\n",
    "                    self.indices[idx + 1 :] if idx < len(tensors) - 1 else []\n",
    "                )\n",
    "\n",
    "                subscript = left_subscript + [self.output] + right_subscript\n",
    "                subscript = \",\".join(subscript) + \"->\" + self.indices[idx]\n",
    "\n",
    "                fn_args = [*left_tensors, tensor, *right_tensors]\n",
    "                return einsum(subscript)(*fn_args)\n",
    "\n",
    "            back_functions.append(back_fn)\n",
    "\n",
    "        return back_functions\n",
    "\n",
    "    def __call__(self, *tensors: Tensor):\n",
    "        result = to_tensor(np.einsum(self.subscript, *tensors))\n",
    "        result.args = tuple(tensors)\n",
    "        result.back_fn = tuple(self._generate_back_fns(tensors))\n",
    "        result.name = f\"einsum {self.subscript}\"\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Proof\n",
    "\n",
    "Let $A, B,...$ be a collection of tensors of arbitrary dimension.\n",
    "\n",
    "Suppose these tensors are combined with a valid einsum operation as follows (in einstein notation):\n",
    "\n",
    "$$z_{z_1, z_2, ...,z_n} = a_{a_1, a_2, ...,a_m}b_{b_1, b_2,...,b_p}...$$\n",
    "\n",
    "Then, because the einsum operation is valid:\n",
    "\n",
    "$$\\{z_1, z_2, ...,z_n\\} \\subset \\{a_1, a_2, ...,a_m,b_1, b_2,...,b_p,...\\}$$\n",
    "\n",
    "That is, every index in the output tensor is included at least once in the set of indices for input tensors.\n",
    "\n",
    "Note here that while the set of indices for each individual tensor contains no duplicates: \n",
    "$$|\\{z_1, z_2, ...,z_n\\}| = n$$\n",
    "$$|\\{a_1, a_2, ...,a_m\\}| = m$$\n",
    "$$|\\{b_1, b_2, ...,b_p\\}| = p$$\n",
    "The same index can appear in the set of indices for multiple input tensors.\n",
    "\n",
    "The einsum operation can be written in terms of an `einsum` function as follows:\n",
    "\n",
    "$$Z = \\texttt{einsum}(a_1, a_2, ...,a_m,b_1, b_2,...,b_p,...\\rightarrow z_1, z_2, ...,z_n)(A, B,...)$$\n",
    "\n",
    "\n",
    "Now suppose that each input is potentially a function of some value $x$: $A(x), B(x), ...$ (although the derivative of each input wrt $x$ could be $0$) . In einstein notation, this becomes:\n",
    "\n",
    "$$z_{z_1, z_2, ...,z_n}(x) = a_{a_1, a_2, ...,a_m}(x)b_{b_1, b_2,...,b_p}(x)...$$\n",
    "\n",
    "Thus, we find \n",
    "$$\\frac{\\partial Z}{\\partial x} = \\frac{\\partial z_{z_1, z_2, ...,z_n}(x)}{\\partial x} = \\frac{\\partial }{\\partial x}\\left ( a_{a_1, a_2, ...,a_m}b_{b_1, b_2,...,b_p}... \\right )$$\n",
    "\n",
    "Because we are using einstein notation, we can apply the product rule to get:\n",
    "\n",
    "$$\\frac{\\partial }{\\partial x}\\left ( a_{a_1, a_2, ...,a_m}b_{b_1, b_2,...,b_p}... \\right ) = \\frac{\\partial a_{a_1, a_2, ...,a_m}(x)}{\\partial x}b_{b_1, b_2,...,b_p}... + a_{a_1, a_2, ...,a_m}(x)\\frac{\\partial b_{b_1, b_2,...,b_p}}{\\partial x}... + ...$$\n",
    "\n",
    "Using the `einstein` function, we can rewrite this as\n",
    "\n",
    "$$\\frac{\\partial Z}{\\partial x}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
